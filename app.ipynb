{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "import torch\n",
    "from facenet_pytorch import InceptionResnetV1\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from transformers import pipeline, AutoTokenizer, AutoModel\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "import glob\n",
    "import pickle\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "class ImageClusteringAlgorithm:\n",
    "    def __init__(self):\n",
    "        self.facenet_model = InceptionResnetV1(pretrained=\"vggface2\").eval().to(device)\n",
    "        self.resnet_model = models.resnet18(pretrained=True).eval().to(device)\n",
    "        self.caption_pipeline = pipeline(\"image-to-text\", model=\"Salesforce/blip-image-captioning-large\", device=device)\n",
    "        self.language_model = AutoModel.from_pretrained(\"distilbert-base-uncased\").to(device)\n",
    "        self.language_tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "        # Define the transformation layers to make the shapes of the output of facenet, resnet, and language models the same\n",
    "        self.face_transform = torch.nn.Linear(512, 1000).to(device)\n",
    "        self.caption_transform = torch.nn.Linear(768, 1000).to(device)\n",
    "\n",
    "    def compute_features(self, image_directory):\n",
    "        self.image_directory = os.path.expanduser(image_directory)\n",
    "        file_extensions = (\"*.jpg\", \"*.jpeg\", \"*.png\", \"*.bmp\", \"*.tiff\", \"*.tif\")\n",
    "        self.image_paths = []\n",
    "        for extension in tqdm(file_extensions, \"Loading image paths\"):\n",
    "            self.image_paths.extend(glob.glob(os.path.join(self.image_directory, extension)))\n",
    "        features = []\n",
    "        for image_path in tqdm(self.image_paths, desc=\"Computing features\"):\n",
    "            feature = self._generate_image_features(image_path)\n",
    "            features.append(feature)\n",
    "        self.features = np.vstack(features)\n",
    "\n",
    "    def compute_clusters(self, num_clusters):\n",
    "        self.num_clusters = num_clusters\n",
    "        print(f\"Clustering images into {self.num_clusters} clusters...\")\n",
    "        kmeans = KMeans(n_clusters=self.num_clusters, random_state=42)\n",
    "        self.labels = kmeans.fit_predict(self.features)\n",
    "        print(\"Computing silhouette score...\")\n",
    "        silhouette_avg = silhouette_score(self.features, self.labels)\n",
    "        print(f\"Silhouette score: {silhouette_avg:.4f}\")\n",
    "        self.clusters = [[] for _ in range(self.labels.max() + 1)]\n",
    "        for idx, label in enumerate(self.labels):\n",
    "            self.clusters[label].append(self.image_paths[idx])\n",
    "\n",
    "    def save_clustered_images(self, output_directory):\n",
    "        output_directory = os.path.expanduser(output_directory)\n",
    "        os.makedirs(output_directory, exist_ok=True)\n",
    "        for cluster_id in range(self.num_clusters):\n",
    "            cluster_dir = os.path.join(output_directory, f\"cluster_{cluster_id}\")\n",
    "            os.makedirs(cluster_dir, exist_ok=True)\n",
    "            for image_path in self.clusters[cluster_id]:\n",
    "                new_filename = os.path.join(cluster_dir, os.path.basename(image_path))\n",
    "                shutil.copyfile(image_path, new_filename)\n",
    "\n",
    "    def _generate_image_features(self, image_path):\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        transform = transforms.Compose([transforms.Resize((512, 512)), transforms.ToTensor()])\n",
    "        image_tensor = transform(image).unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "            resnet_features = self.resnet_model(image_tensor)\n",
    "            face_features = self.face_transform(self.facenet_model(image_tensor))\n",
    "            caption_features = self.caption_transform(self._generate_caption(image_path))\n",
    "        features = np.concatenate((face_features.cpu().detach().numpy(), resnet_features.cpu().detach().numpy(), caption_features.cpu().detach().numpy()))\n",
    "        return features\n",
    "\n",
    "    def _generate_caption(self, image_path):\n",
    "        caption = self.caption_pipeline(image_path)[0][\"generated_text\"]\n",
    "        tokens = self.language_tokenizer.encode(caption, return_tensors=\"pt\").to(device)\n",
    "        vector = self.language_model(tokens).last_hidden_state.mean(dim=1)\n",
    "        return vector\n",
    "    \n",
    "    def save_features(self, file_name):\n",
    "        data = { \"features\": self.features, \"image_paths\": self.image_paths, \"image_directory\": self.image_directory }\n",
    "        with open(file_name, \"wb\") as file: \n",
    "            pickle.dump(data, file)\n",
    "    \n",
    "    def load_features(self, file_name):\n",
    "        with open(file_name, \"b\") as file:\n",
    "            data = pickle.load(file)\n",
    "\n",
    "        ica = ImageClusteringAlgorithm()\n",
    "        ica.features = data[\"features\"]\n",
    "        ica.image_paths = data[\"image_paths\"]\n",
    "        ica.image_directory = data[\"image_directory\"]\n",
    "\n",
    "    def save_clusters(self, file_name):\n",
    "        data = { \"num_clusters\": self.num_clusters, \"clusters\": self.clusters }\n",
    "        with open(file_name, \"wb\") as file: \n",
    "            pickle.dump(data, file)\n",
    "\n",
    "    def load_features(self, file_name):\n",
    "        with open(file_name, \"b\") as file:\n",
    "            data = pickle.load(file)\n",
    "\n",
    "        ica = ImageClusteringAlgorithm()\n",
    "        ica.num_clusters = data[\"clusters\"]\n",
    "        ica.clusters = data[\"clusters\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alexanderschiffhauer/.local/share/virtualenvs/clustering-NSSxPA_6/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/alexanderschiffhauer/.local/share/virtualenvs/clustering-NSSxPA_6/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_transform.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Loading image paths: 100%|██████████| 6/6 [00:00<00:00, 155.80it/s]\n",
      "Computing features:   0%|          | 0/8617 [00:00<?, ?it/s]/Users/alexanderschiffhauer/.local/share/virtualenvs/clustering-NSSxPA_6/lib/python3.11/site-packages/transformers/generation/utils.py:1313: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Computing features:   0%|          | 9/8617 [00:18<4:47:33,  2.00s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m clusters_file \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m~/Downloads/JPGs/ImageClusteringAlgorithmClusters.pkl\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mexists(features_file):\n\u001b[0;32m----> 6\u001b[0m     ica\u001b[39m.\u001b[39;49mcompute_features(image_directory)\n\u001b[1;32m      7\u001b[0m     ica\u001b[39m.\u001b[39msave_features(features_file)\n\u001b[1;32m      8\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "Cell \u001b[0;32mIn[22], line 38\u001b[0m, in \u001b[0;36mImageClusteringAlgorithm.compute_features\u001b[0;34m(self, image_directory)\u001b[0m\n\u001b[1;32m     36\u001b[0m features \u001b[39m=\u001b[39m []\n\u001b[1;32m     37\u001b[0m \u001b[39mfor\u001b[39;00m image_path \u001b[39min\u001b[39;00m tqdm(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimage_paths, desc\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mComputing features\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m---> 38\u001b[0m     feature \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_generate_image_features(image_path)\n\u001b[1;32m     39\u001b[0m     features\u001b[39m.\u001b[39mappend(feature)\n\u001b[1;32m     40\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeatures \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mvstack(features)\n",
      "Cell \u001b[0;32mIn[22], line 71\u001b[0m, in \u001b[0;36mImageClusteringAlgorithm._generate_image_features\u001b[0;34m(self, image_path)\u001b[0m\n\u001b[1;32m     69\u001b[0m     resnet_features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresnet_model(image_tensor)\n\u001b[1;32m     70\u001b[0m     face_features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mface_transform(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfacenet_model(image_tensor))\n\u001b[0;32m---> 71\u001b[0m     caption_features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcaption_transform(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_generate_caption(image_path))\n\u001b[1;32m     72\u001b[0m features \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mconcatenate((face_features\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mnumpy(), resnet_features\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mnumpy(), caption_features\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mnumpy()))\n\u001b[1;32m     73\u001b[0m \u001b[39mreturn\u001b[39;00m features\n",
      "Cell \u001b[0;32mIn[22], line 76\u001b[0m, in \u001b[0;36mImageClusteringAlgorithm._generate_caption\u001b[0;34m(self, image_path)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_generate_caption\u001b[39m(\u001b[39mself\u001b[39m, image_path):\n\u001b[0;32m---> 76\u001b[0m     caption \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcaption_pipeline(image_path)[\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mgenerated_text\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m     77\u001b[0m     tokens \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlanguage_tokenizer\u001b[39m.\u001b[39mencode(caption, return_tensors\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     78\u001b[0m     vector \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlanguage_model(tokens)\u001b[39m.\u001b[39mlast_hidden_state\u001b[39m.\u001b[39mmean(dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/clustering-NSSxPA_6/lib/python3.11/site-packages/transformers/pipelines/image_to_text.py:99\u001b[0m, in \u001b[0;36mImageToTextPipeline.__call__\u001b[0;34m(self, images, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, images: Union[\u001b[39mstr\u001b[39m, List[\u001b[39mstr\u001b[39m], \u001b[39m\"\u001b[39m\u001b[39mImage.Image\u001b[39m\u001b[39m\"\u001b[39m, List[\u001b[39m\"\u001b[39m\u001b[39mImage.Image\u001b[39m\u001b[39m\"\u001b[39m]], \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     75\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     76\u001b[0m \u001b[39m    Assign labels to the image(s) passed as inputs.\u001b[39;00m\n\u001b[1;32m     77\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[39m        - **generated_text** (`str`) -- The generated text.\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 99\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(images, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/clustering-NSSxPA_6/lib/python3.11/site-packages/transformers/pipelines/base.py:1109\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1101\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mnext\u001b[39m(\n\u001b[1;32m   1102\u001b[0m         \u001b[39miter\u001b[39m(\n\u001b[1;32m   1103\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_iterator(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1106\u001b[0m         )\n\u001b[1;32m   1107\u001b[0m     )\n\u001b[1;32m   1108\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1109\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrun_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/clustering-NSSxPA_6/lib/python3.11/site-packages/transformers/pipelines/base.py:1116\u001b[0m, in \u001b[0;36mPipeline.run_single\u001b[0;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[1;32m   1114\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrun_single\u001b[39m(\u001b[39mself\u001b[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001b[1;32m   1115\u001b[0m     model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpreprocess(inputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpreprocess_params)\n\u001b[0;32m-> 1116\u001b[0m     model_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(model_inputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mforward_params)\n\u001b[1;32m   1117\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpostprocess(model_outputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpostprocess_params)\n\u001b[1;32m   1118\u001b[0m     \u001b[39mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/clustering-NSSxPA_6/lib/python3.11/site-packages/transformers/pipelines/base.py:1015\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m   1013\u001b[0m     \u001b[39mwith\u001b[39;00m inference_context():\n\u001b[1;32m   1014\u001b[0m         model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m-> 1015\u001b[0m         model_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_forward(model_inputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mforward_params)\n\u001b[1;32m   1016\u001b[0m         model_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mdevice(\u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[1;32m   1017\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/clustering-NSSxPA_6/lib/python3.11/site-packages/transformers/pipelines/image_to_text.py:114\u001b[0m, in \u001b[0;36mImageToTextPipeline._forward\u001b[0;34m(self, model_inputs, generate_kwargs)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[39m# FIXME: We need to pop here due to a difference in how `generation.py` and `generation.tf_utils.py`\u001b[39;00m\n\u001b[1;32m    110\u001b[0m \u001b[39m#  parse inputs. In the Tensorflow version, `generate` raises an error if we don't use `input_ids` whereas\u001b[39;00m\n\u001b[1;32m    111\u001b[0m \u001b[39m#  the PyTorch version matches it with `self.model.main_input_name` or `self.model.encoder.main_input_name`\u001b[39;00m\n\u001b[1;32m    112\u001b[0m \u001b[39m#  in the `_prepare_model_inputs` method.\u001b[39;00m\n\u001b[1;32m    113\u001b[0m inputs \u001b[39m=\u001b[39m model_inputs\u001b[39m.\u001b[39mpop(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mmain_input_name)\n\u001b[0;32m--> 114\u001b[0m model_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mgenerate(inputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_inputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mgenerate_kwargs)\n\u001b[1;32m    115\u001b[0m \u001b[39mreturn\u001b[39;00m model_outputs\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/clustering-NSSxPA_6/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/clustering-NSSxPA_6/lib/python3.11/site-packages/transformers/models/blip/modeling_blip.py:1063\u001b[0m, in \u001b[0;36mBlipForConditionalGeneration.generate\u001b[0;34m(self, pixel_values, input_ids, attention_mask, **generate_kwargs)\u001b[0m\n\u001b[1;32m   1030\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1031\u001b[0m \u001b[39mOverrides *generate* function to be able to use the model as a conditional generator\u001b[39;00m\n\u001b[1;32m   1032\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1059\u001b[0m \u001b[39m```\u001b[39;00m\n\u001b[1;32m   1060\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1062\u001b[0m batch_size \u001b[39m=\u001b[39m pixel_values\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n\u001b[0;32m-> 1063\u001b[0m vision_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvision_model(pixel_values\u001b[39m=\u001b[39;49mpixel_values)\n\u001b[1;32m   1065\u001b[0m image_embeds \u001b[39m=\u001b[39m vision_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1067\u001b[0m image_attention_mask \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mones(image_embeds\u001b[39m.\u001b[39msize()[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m], dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mlong)\u001b[39m.\u001b[39mto(image_embeds\u001b[39m.\u001b[39mdevice)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/clustering-NSSxPA_6/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/clustering-NSSxPA_6/lib/python3.11/site-packages/transformers/models/blip/modeling_blip.py:689\u001b[0m, in \u001b[0;36mBlipVisionModel.forward\u001b[0;34m(self, pixel_values, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    685\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mYou have to specify pixel_values\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    687\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings(pixel_values)\n\u001b[0;32m--> 689\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(\n\u001b[1;32m    690\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49mhidden_states,\n\u001b[1;32m    691\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    692\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m    693\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m    694\u001b[0m )\n\u001b[1;32m    696\u001b[0m last_hidden_state \u001b[39m=\u001b[39m encoder_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    697\u001b[0m last_hidden_state \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpost_layernorm(last_hidden_state)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/clustering-NSSxPA_6/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/clustering-NSSxPA_6/lib/python3.11/site-packages/transformers/models/blip/modeling_blip.py:629\u001b[0m, in \u001b[0;36mBlipEncoder.forward\u001b[0;34m(self, inputs_embeds, attention_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    623\u001b[0m     layer_outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[1;32m    624\u001b[0m         create_custom_forward(encoder_layer),\n\u001b[1;32m    625\u001b[0m         hidden_states,\n\u001b[1;32m    626\u001b[0m         attention_mask,\n\u001b[1;32m    627\u001b[0m     )\n\u001b[1;32m    628\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 629\u001b[0m     layer_outputs \u001b[39m=\u001b[39m encoder_layer(\n\u001b[1;32m    630\u001b[0m         hidden_states,\n\u001b[1;32m    631\u001b[0m         attention_mask,\n\u001b[1;32m    632\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    633\u001b[0m     )\n\u001b[1;32m    635\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    637\u001b[0m \u001b[39mif\u001b[39;00m output_attentions:\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/clustering-NSSxPA_6/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/clustering-NSSxPA_6/lib/python3.11/site-packages/transformers/models/blip/modeling_blip.py:403\u001b[0m, in \u001b[0;36mBlipEncoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, output_attentions)\u001b[0m\n\u001b[1;32m    401\u001b[0m residual \u001b[39m=\u001b[39m hidden_states\n\u001b[1;32m    402\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer_norm2(hidden_states)\n\u001b[0;32m--> 403\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmlp(hidden_states)\n\u001b[1;32m    405\u001b[0m hidden_states \u001b[39m=\u001b[39m hidden_states \u001b[39m+\u001b[39m residual\n\u001b[1;32m    407\u001b[0m outputs \u001b[39m=\u001b[39m (hidden_states,)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/clustering-NSSxPA_6/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/clustering-NSSxPA_6/lib/python3.11/site-packages/transformers/models/blip/modeling_blip.py:363\u001b[0m, in \u001b[0;36mBlipMLP.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    361\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc1(hidden_states)\n\u001b[1;32m    362\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mactivation_fn(hidden_states)\n\u001b[0;32m--> 363\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfc2(hidden_states)\n\u001b[1;32m    364\u001b[0m \u001b[39mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/clustering-NSSxPA_6/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/clustering-NSSxPA_6/lib/python3.11/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ica = ImageClusteringAlgorithm()\n",
    "image_directory = \"~/Downloads/JPGs\"\n",
    "features_file = \"~/Downloads/JPGs/ImageClusteringAlgorithmFeatures.pkl\"\n",
    "clusters_file = \"~/Downloads/JPGs/ImageClusteringAlgorithmClusters.pkl\"\n",
    "if not os.path.exists(features_file):\n",
    "    ica.compute_features(image_directory)\n",
    "    ica.save_features(features_file)\n",
    "else:\n",
    "    ica.load_features(features_file)\n",
    "if not os.path.exists(clusters_file):\n",
    "    num_clusters = 50\n",
    "    ica.compute_clusters(num_clusters)\n",
    "    ica.save_clusters(clusters_file)\n",
    "else:\n",
    "    ica.load_clusters(clusters_file)\n",
    "ica.save_clustered_images(\"~/Downloads/ClusteredJPGs\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clustering-NSSxPA_6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
